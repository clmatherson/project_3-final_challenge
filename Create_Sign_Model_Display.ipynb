{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources Used\n",
    "- wget.download('https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/da4babe668a8afb093cc7776d7e630f3/generate_tfrecord.py')\n",
    "- Setup https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'Tensorflow/workspace'\n",
    "SCRIPTS_PATH = 'Tensorflow/scripts'\n",
    "APIMODEL_PATH = 'Tensorflow/models'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'\n",
    "# create pipeline.config Tensorflow/models/my_ssd_mobnet/pipeline.config paste '\n",
    "# delete frow 172 fine_tune_checkpoint_version: V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the labels for each word we are creating\n",
    "labels = [{'name':'book', 'id':1}, \n",
    "          {'name':'doctor', 'id':2},\n",
    "         {'name':'down', 'id':3},\n",
    "          {'name':'drink', 'id':4},\n",
    "          {'name':'eat', 'id':5},\n",
    "          {'name':'fork', 'id':6},\n",
    "          {'name':'go', 'id':7},\n",
    "          {'name':'great', 'id':8},\n",
    "          {'name':'hello', 'id':9},\n",
    "          {'name':'help', 'id':10},\n",
    "          {'name':'i_love_you', 'id':11},\n",
    "          {'name':'more', 'id':12},\n",
    "          {'name':'name', 'id':13},\n",
    "          {'name':'no', 'id':14},\n",
    "          {'name':'police', 'id':15},\n",
    "          {'name':'quiet', 'id':16},\n",
    "          {'name':'stop', 'id':17},\n",
    "          {'name':'up', 'id':18},\n",
    "          {'name':'world', 'id':19},\n",
    "          {'name':'yes', 'id':20},\n",
    "         ]\n",
    "\n",
    "#the label map for above labels for TenselFlow Library detection\n",
    "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:  #create folder for records at label.map in annotation file\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create TensorFlow records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: Tensorflow/workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: Tensorflow/workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "# Ensures data is in the correct format. exuecutes cammand inside our notebook\n",
    "# https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html\n",
    "# load $ pip install tensorflow-object-detection-api\n",
    "# /generate_tfrecord.py allows us to pass variables from notebook to command line\n",
    "# training images and test images \n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download TF Models Pretrained Models from Tensorflow Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this only needs to be done once, clowns down files from githup to train the model\n",
    "# !cd Tensorflow && git clone https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is code to download a different starting model from TensorFlow 2 Detection Model Zoo \n",
    "# wget.download('http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz')\n",
    "# !mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz {PRETRAINED_MODEL_PATH}\n",
    "# !cd {PRETRAINED_MODEL_PATH} && tar -zxvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Copy Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new folder\n",
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model leveraging that, uses pretrained paths from above, \n",
    "\n",
    "# !mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\n",
    "# !cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Update Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util # pip install tensorflow-object-detection-api\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'\n",
    "config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 20\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 320\n",
       "       width: 320\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 3.9999998989515007e-05\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         random_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.009999999776482582\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     use_depthwise: true\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       additional_layer_depth: 128\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 3.9999998989515007e-05\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 128\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "       share_prediction_tower: true\n",
       "       use_depthwise: true\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "       use_static_shapes: false\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 4\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.07999999821186066\n",
       "         total_steps: 50000\n",
       "         warmup_learning_rate: 0.026666000485420227\n",
       "         warmup_steps: 1000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
       " num_steps: 50000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"detection\"\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/train.record\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       " }}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_text = text_format.MessageToString(pipeline_config)\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = 20  # !!!!! need to update this to number of words (objects)\n",
    "pipeline_config.train_config.batch_size = 4  #  can be higher if bigger GPU\n",
    "pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\n",
    "pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=10000\n"
     ]
    }
   ],
   "source": [
    "# step 1 change this from 5,000 to 10,000 to 20,000\n",
    "# step 2 copy this into cd RealTimeObjectDirection and train model into command prompt\n",
    "# look for loss 0.099\n",
    "\n",
    "print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=10000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Load Train Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restarted kernel\n",
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-1')).expect_partial()  # ckpt is from Tensorflow/workplace/models/my_ssd... folder\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    print(i)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Detect in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'id': 1, 'name': 'book'},\n",
       " 2: {'id': 2, 'name': 'doctor'},\n",
       " 3: {'id': 3, 'name': 'down'},\n",
       " 4: {'id': 4, 'name': 'drink'},\n",
       " 5: {'id': 5, 'name': 'eat'},\n",
       " 6: {'id': 6, 'name': 'fork'},\n",
       " 7: {'id': 7, 'name': 'go'},\n",
       " 8: {'id': 8, 'name': 'great'},\n",
       " 9: {'id': 9, 'name': 'hello'},\n",
       " 10: {'id': 10, 'name': 'help'},\n",
       " 11: {'id': 11, 'name': 'i_love_you'},\n",
       " 12: {'id': 12, 'name': 'more'},\n",
       " 13: {'id': 13, 'name': 'name'},\n",
       " 14: {'id': 14, 'name': 'no'},\n",
       " 15: {'id': 15, 'name': 'police'},\n",
       " 16: {'id': 16, 'name': 'quiet'},\n",
       " 17: {'id': 17, 'name': 'stop'},\n",
       " 18: {'id': 18, 'name': 'up'},\n",
       " 19: {'id': 19, 'name': 'world'},\n",
       " 20: {'id': 20, 'name': 'yes'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')\n",
    "category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap.release()   # this can be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "# Setup capture\n",
    "cap = cv2.VideoCapture(0)   # access webcam\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  \n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(width)\n",
    "print(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "test1 0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-cl8wq7nq\\opencv\\modules\\highgui\\src\\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-bc6e5f0b34a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#Creating a new environment and installing opencv-python by pip worked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#     plt.imshow(image_np_with_detections) #,  cv2.resize(image_np_with_detections, (800, 600)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object detection'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np_with_detections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# display transformed array back onto screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m#     cv2.imshow('object detection', cv2.resize(image_np, (800,600)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-cl8wq7nq\\opencv\\modules\\highgui\\src\\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "while (True): \n",
    "    ret, frame = cap.read()  # get image from camera\n",
    "    image_np = np.array(frame)  # convert to numpy Array\n",
    "    \n",
    "    # pass in np.expand_dims to place in another array\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32) # convert to tensor flwo tensor\n",
    "    detections = detect_fn(input_tensor)  # make the detection from function above in step 7\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))  # the number of detections\n",
    "    \n",
    "    detections = {key: value[0, :num_detections].numpy()     #preprocessing\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64) # collect the dectection classes\n",
    "\n",
    "    label_id_offset = 1   # start at 1 and not 0 because our category index (list of words) starts at 1\n",
    "    image_np_with_detections = image_np.copy()  # make a copy as backup of raw image from webcam\n",
    "    print('test1', i)\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(   # transform array, place score and box on image\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,  # position box correctly\n",
    "                max_boxes_to_draw=1,   # number of boxes to draw\n",
    "                min_score_thresh=0.3,   # miminum score threshold\n",
    "                agnostic_mode=False) # not to differ between post and neg\n",
    "\n",
    "#     img = cv2.imread('\\Tensorflow\\workspace\\images\\test\\hello.d06b8f38-6e15-11eb-9de2-0c7a15e4857b.jpg',0)\n",
    "#     cv2.imshow('image', img)\n",
    "    \n",
    "  #This problem says that an assertion is failed. If the file doesn't exist in the given path,cv2 returns this error. So check if the file exists in your given path.  \n",
    "#Creating a new environment and installing opencv-python by pip worked\n",
    "#     plt.imshow(image_np_with_detections) #,  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))  # display transformed array back onto screen\n",
    "#     cv2.imshow('object detection', cv2.resize(image_np, (800,600)))\n",
    "   \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # break out of loop\n",
    "        cap.release()\n",
    "        break\n",
    "# https://stackoverflow.com/questions/28776053/opencv-gtk2-x-error\n",
    "# for windows just uninstall the OpenCV pip uninstall opencv-python and reinstall pip install opencv-python        \n",
    "        \n",
    "        \n",
    "# sudo apt-get purge python-opencv\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install python-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get purge python-opencv\n",
    "sudo apt-get update\n",
    "sudo apt-get install python-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__\n",
    "# cv.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))  # display transformed array back onto screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose this was the source \n",
    "cap = cv2.VideoCapture('data/input.mpg')\n",
    "# Get width and height of the frame of video\n",
    "width  = cap.get(3) # float width\n",
    "height = cap.get(4) # float height    \n",
    "# Make a video writer to see if video being taken as input inflict any changes you make\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "out_video = cv2.VideoWriter('result/output.avi', fourcc, 20.0, (int(width), int(height)), True)\n",
    "# Then try this\n",
    "while(cap.isOpened()):\n",
    "        # Read each frame where ret is a return boollean value(True or False)\n",
    "        ret, frame = cap.read()\n",
    "        # if return is true continue because if it isn't then frame is of NoneType in that case you cant work on that frame\n",
    "        if ret:\n",
    "            # Any preprocessing you want to make on the frame goes here\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # See if preprocessing is working on the frame it should\n",
    "            cv2.imshow('frame', gray)\n",
    "            # finally write your preprocessed frame into your output video\n",
    "            out_video.write(gray) # write the modifies frame into output video \n",
    "            # to forcefully stop the running loop and break out, if it doesnt work use ctlr+c\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "       # break out if frame has return NoneType this means that once script encounters such frame it breaks out \n",
    "       # You will get the error otherwise \n",
    "        else:\n",
    "            break\n",
    "# this will tell you from which frame the video is corrupted and need to be changed \n",
    "# this could be due to an empty frame. Empty frame might be a result of mistake made while creating or editing video in tools #like adobe premier pro or other products alike \n",
    "# You can check you output video it will contains all frame before the error is encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "img = cv2.imread('messi5.jpg',0)\n",
    "cv2.imshow('image',img)\n",
    "k = cv2.waitKey(0)\n",
    "if k == 27:         # wait for ESC key to exit\n",
    "    cv2.destroyAllWindows()\n",
    "elif k == ord('s'): # wait for 's' key to save and exit\n",
    "    cv2.imwrite('messigray.png',img)\n",
    "    cv2.destroyAllWindows()\n",
    "# same error but in imread   https://stackoverflow.com/questions/50783177/opencv-the-function-is-not-implemented-rebuild-the-library-with-windows\n",
    "# 7:39 am pip install opencv-python   \n",
    "# pip install opencv-contrib-python\n",
    "\n",
    "# get CV2 info:  python -c \"import cv2; print(cv2.getBuildInformation())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=IOI0o3Cxv9Q\n",
    "\n",
    "Hi, sir. Me again from the yesterday instllation video. \n",
    "I have gone through this video and i have train my model. \n",
    "But there is an error in the final step which is  cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "and it gives the error like this\n",
    "error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-memyuvq3\\opencv\\modules\\highgui\\src\\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
    "\n",
    "I think i need to install opencv but how to install and which drive or location i supposed to install it?\n",
    "Hi, sir. Me again from the yesterday instllation video. \n",
    "I have gone through this video and i have train my model. \n",
    "But there is an error in the final step which is  cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "and it gives the error like this\n",
    "error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-memyuvq3\\opencv\\modules\\highgui\\src\\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
    "\n",
    "I think i need to install opencv but how to install and which drive or location i supposed to install it?\n",
    "\n",
    "JiaJun Loh\n",
    "2 weeks ago (edited)\n",
    "Hi sir, i finally solved the problem with pip install --user opencv-contrib-python. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To anyone in the future: I had to put opencv_ffmpeg310.dll into my working directory. Otherwise the VideoCapture basically just failed silently. â€“ MrZander Jan 28 '19 at 23:21"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
